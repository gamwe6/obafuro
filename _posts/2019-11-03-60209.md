---
layout: post
title: "scheduleで実行時間を自動化し、スクレイピング"
date: 2019-11-03 04:39:07
categories: python web-scraping
---
<p>毎日一定の時刻に、特定のURLからjsonを取得しDataframeに保存する、ということがやりたいです。<br>
テストとして１分置きでの取得条件で書いてみたのですが、実行すると以下のエラーが表示されてしまいます。</p>

<pre><code>TypeError: get_all_reviews() missing 1 required positional argument: 'url'
</code></pre>

<p>スクリプト（schedule部分のスクリプトを削除すると、正常に実行されます）</p>

<pre class="lang-py prettyprint-override"><code>def get_all_reviews(url):
    rvw_list_text = []
    i = 1
    while True:
        print(i,'searching')
        i += 1        
        res = requests.get(url)
        res_json = json.loads(res.text)
        data = res_json.get('data')
        for d in data:
            rvw_list_text.append(d)

        #dataが殻でなければ
        if data !=[]:
            paging = dict(res_json.get('paging'))
            next_page = str(paging['next'])
            url = next_page
        else:
            break


    return rvw_list_text

if __name__ == '__main__':
    url = "https://graph.facebook.com/xxx"
    rvw_list_text = get_all_reviews(url)


#AM10:30にjobを実行
#schedule.every().day.at("10:30").do(get_all_reviews)
schedule.every(1).minutes.do(get_all_reviews)

while True:
    schedule.run_pending()
    time.sleep(1)
</code></pre>
