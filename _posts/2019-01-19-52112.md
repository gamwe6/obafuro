---
layout: post
title: MNISTを利用したKerasによるVAEにおいて、epochごとのkl_lossの係数変化を出力させて確認する方法
date: 2019-01-19 07:05:17
categories: python 機械学習 深層学習 keras
---
<!-- {% raw %} -->
<p>私はKerasという深層学習フレームワークを使って以下のようにepochごとにkl_lossの係数<code>-aneeling_callback.variable</code>が変化するような深層学習を走らせようとしています。<br>
きちんとkl_lossの係数がepochごとに変化しているのかを確認するために以下のようなコード<code>print(K.eval(self.variable))</code>を書いたのですが、エラーが出てしまいます。<br>
以下のコードの範疇においてkl_lossの係数を出力する方法に心当たりのある方がいればご教授いただけると助かります。</p>

<p>コード</p>

<pre><code>#KL-divergence annealingのメモ(先にこちらを実行してから上のコードを実行する事)
from keras import backend as K

#hp_lambdaはkl_lossの係数として用意する。
hp_lambda = K.variable(0)  # default values

from keras import callbacks

class AneelingCallback(callbacks.Callback): #callbacks.Callbackはhttps://keras.io/ja/callbacks/#callbackを参照したい。
    '''Aneeling theano shared variable.
        # Arguments
        schedule(関数): a function that takes an epoch index as input
        (integer, indexed from 0) and returns a new
        learning rate as output (float).
        '''
    def __init__(self, schedule, variable):
        super(AneelingCallback, self).__init__()
        self.schedule = schedule
        self.variable = variable #hp_lambdaにあたるもの

    def on_epoch_begin(self, epoch, logs={}):
        assert hasattr(self.model.optimizer, 'lr'), \
            'Optimizer must have a "lr" attribute.'
        value = self.schedule(epoch)
        assert type(value) == float, 'The output of the "schedule" function should be float.'
        K.set_value(self.variable, value) #上のvalueで得た値をvariableにセットする。
　　　　 print(K.eval(self.variable)) #後で消す行。kl_lossの係数をepochごとに知りたい...。

def schedule(epoch):
    return 0.5 * epoch

aneeling_callback = AneelingCallback(schedule, hp_lambda)

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from keras.layers import Lambda
from keras.losses import mse, binary_crossentropy
from keras.utils import plot_model

import matplotlib.pyplot as plt
import argparse
import os


# reparameterization trick
# instead of sampling from Q(z|X), sample eps = N(0,I)
# z = z_mean + sqrt(var)*eps
def sampling(args):
    """Reparameterization trick by sampling fr an isotropic unit Gaussian.
    # Arguments:
        args (tensor): mean and log of variance of Q(z|X)
    # Returns:
        z (tensor): sampled latent vector
    """

    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    dim = K.int_shape(z_mean)[1]
    # by default, random_normal has mean=0 and std=1.0
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon
  #関数定義終わり




from keras.datasets import mnist 
    import numpy as np
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

image_size = x_train.shape[1]
original_dim = image_size * image_size
x_train = np.reshape(x_train, [-1, original_dim])
x_test = np.reshape(x_test, [-1, original_dim])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

from keras.models import Model
from keras.layers import Input, Dense
# network parameters
input_shape = (original_dim, )

intermediate_dim = 256
batch_size = 8
latent_dim = 128
epochs = 1 #とりあえず

# VAE model = encoder + decoder
# build encoder model
inputs = Input(shape=input_shape, name='encoder_input')
x = Dense(intermediate_dim, activation='relu')(inputs)
z_mean = Dense(latent_dim, name='z_mean')(x)
z_log_var = Dense(latent_dim, name='z_log_var')(x)

z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

# instantiate encoder model
encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')

# build decoder model
latent_inputs = Input(shape=(latent_dim,), name='z_sampling')
x = Dense(intermediate_dim, activation='relu')(latent_inputs)
outputs = Dense(original_dim, activation='sigmoid')(x)

# instantiate decoder model
decoder = Model(latent_inputs, outputs, name='decoder')

# instantiate VAE model
#encoder(inputs)[2]はzのこと
outputs = decoder(encoder(inputs)[2])
vae = Model(inputs, outputs, name='vae_mlp')

# VAE loss = mse_loss or xent_loss + kl_loss
reconstruction_loss = binary_crossentropy(inputs,
                                            outputs)
reconstruction_loss *= original_dim
kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
kl_loss = K.sum(kl_loss, axis=-1)
kl_loss *= -aneeling_callback.variable #hp_lambda...?
# VAE loss = mse_loss or xent_loss + kl_loss
vae_loss = K.mean(reconstruction_loss + kl_loss)
vae.add_loss(vae_loss)
vae.compile(optimizer='adam')

print(vae.summary())

# train the autoencoder
vae.fit(x_train,
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[aneeling_callback],
        validation_data=(x_test, None))
</code></pre>

<p>エラー</p>

<pre><code>line 25
    K.eval(self.variable) #後で消す行
    ^
SyntaxError: invalid character in identifier
</code></pre>
<!-- {% endraw %} -->
