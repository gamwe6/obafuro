---
layout: post
title: ゼロから作るDeep Learning：定数関数の微分
date: 2017-07-14 10:46:39
categories: python 深層学習 機械学習
---
<p>オライリー・ジャパンの「ゼロから作る Deep Learning」という本についての質問です。</p>

<p>疑問は、「定数関数を微分してるから結果はゼロになるんじゃないか？」というものですが、とりあえず関数を見てください。本を持ってる人は、p.159ページです。</p>

```
def loss(self, x, t):
    y = self.predict(x)
    return self.lastLayer.forward(y, t)

def numerical_gradient(self, x, t):
    loss_W = lambda W: self.loss(x, t)

    grads = {}
    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

    return grads
```

<p>問題はnumerical_gradientで、これにxとtを代入すると、まずself.loss(x, t)が定数として確定します(これをcとおきます)。次に、loss_W = lambda W: self.loss(x, t)は、Wに関係なく定数cを返す定数関数として確定します。これに対してnumerical_gradientをしてもcの微分なので0しか返ってこないのでは？</p>

<p>numerical_gradientの中身は次のようになります。</p>

```
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)

    for idx in range(x.size):
        tmp_val=x[idx]
        # f(x+h)の計算
        x[idx] = tmp_val + h
        fxh1 = f(x)

        #f(x-h)の計算
        x[idx] = tmp_val - h
        fxh2 = f(x)

        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val

    return grad
```

<p>つまり、numerical_gradient(loss_W, self.params['W1'])は、loss_W(Wij+h)やloss_W(Wij-h)を計算して(loss_W(Wij+h) - loss_W(Wij-h))/ (2*h)とかを出力しますが、loss_W(Wij+h)=loss_W(Wij-h)=self.loss(x, t)=cなので、ゼロしか返さないのでは？ということです。</p>

<p>わかる方お願いします！</p>
