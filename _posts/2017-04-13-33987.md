---
layout: post
title: Logistic Regression実装について
date: 2017-04-13 15:23:34
categories: python 機械学習
---
<!-- {% raw %} -->
<p>4.62365962451697,78.0246928153624,0<br>
↑のような形式で99行あるex2data1.txtを読み込んで、このファイルに対してロジスティック回帰分析をpythonで行いたいのですが、学習率α等を変えても何故か図のような結果になってしまいます。間違っているところがありましたら指摘していただきたく存じます。</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

def sigmoid(X):
    return 1/(1 + np.exp(-X))
data = pd.read_csv("ex2data1.txt", header=None)
X = np.array([data[0],data[1]]).T
y = np.array(data[2])

O = np.ones(len(y))
m = float(len(y))
Xm = X/m
X1 = np.c_[O,Xm]

pos = (y==1)
neg = (y==0)
plt.scatter(X[pos,0], X[pos,1], marker='+', c='b')
plt.scatter(X[neg,0], X[neg,1], marker='o', c='y')
plt.legend(['Admitted', 'Not admitted'], scatterpoints=1)
plt.xlabel("Exam 1 Score")
plt.ylabel("Exam 2 Score")

alpha = 0.5
theta = np.array([1,1,1])

for i in range(1000): #gradient
    h = sigmoid(np.inner(theta, X1))
    theta1_tmp = 1/m * np.sum((h-y)*X1[:, 0])
    theta2_tmp = 1/m * np.sum((h-y)*X1[:, 1])
    theta3_tmp = 1/m * np.sum((h-y)*X1[:, 2])
    theta = theta - alpha*np.array([theta1_tmp, theta2_tmp, theta3_tmp])

theta0 = theta[0]
theta1 = theta[1]
theta2 = theta[2]

plot_x = np.array([min(X[:,0])-2, max(X[:,0])+2])
plot_y = - (theta0 + theta1*plot_x) / theta2
plt.plot(plot_x, plot_y, 'b')

plt.show()
</code></pre>

<p><a href="https://i.stack.imgur.com/g5MNK.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/g5MNK.png" alt="出力結果"></a></p>
<!-- {% endraw %} -->
