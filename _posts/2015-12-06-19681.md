---
layout: post
title: nokogiriで取得したいオブジェクトをすべて取得できない
date: 2015-12-06 09:00:19
categories: ruby web-scraping
---
<p>下記ページのHTMLには、<code>a[class ^= "history"]</code> オブジェクトが36個あることを確認しています。<br>
<a href="http://ja.aliexpress.com/wholesale?catId=0&amp;initiative_id=AS_20151205231131&amp;SearchText=bag+mens" rel="nofollow">http://ja.aliexpress.com/wholesale?catId=0&amp;initiative_id=AS_20151205231131&amp;SearchText=bag+mens</a></p>

<p>しかし、nokogiriのcssセレクタで、上記オブジェクトを取得すると4個しか取得できません。</p>

<p>原因がわからず、大変困っています。<br>
原因がわかるかた、いらっしゃいますでしょうか？</p>

```
require 'open-uri'
require 'nokogiri'
require 'pstore'
require 'anemone'
require 'open_uri_redirections'
require 'openssl'

class URLList
  def GetHtmlObj(url)
    charset = nil
    html = open(url, :allow_redirections =&gt; :safe, :ssl_verify_mode =&gt; OpenSSL::SSL::VERIFY_NONE) do |f|
      charset = f.charset # 文字種別を取得
      f.read # htmlを読み込んで変数htmlに渡す
    end
    # htmlをパース(解析)してオブジェクトを作成
    return Nokogiri::HTML.parse(html, nil, charset)
  end

  def GetURLList(url)
    url_list = Array.new
    doc = GetHtmlObj(url)
    doc.css('div[class = "ui-pagination-navi util-left"]&gt;a').each do |page|
      url_list &lt;&lt; page.attribute("href").text 
    end
    url_list = url_list.sort_by do |k|
      /(.+)(page=)([\d]+)/ =~ k
      $3.to_i
    end
    return url_list.uniq
  end

  def GetURLListAll(url,n)
    url_list_all = Array.new
    crawl_url = URLList.new
    last_url = url

    n.times do |n|
      url_list = crawl_url.GetURLList(last_url)
      last_url = url_list.last

      url_list_all.concat(url_list)
    end    
     url_list_all = url_list_all.sort_by do |k|
      /(.+)(page=)([\d]+)/ =~ k
      $3.to_i
    end
    return url_list_all.uniq
  end

  def GetProductURLList2(url)
    url_list = Array.new
    doc = GetHtmlObj(url)
    doc.css('a[class = "history-item product "]').each do |page|
      url_list &lt;&lt; page.attribute("href").text 
    end

    return url_list
  end

  def GetProductURLList(url)
    url_list = Array.new
    doc = GetHtmlObj(url)
    num=0
    doc.css('a[class ^= "history"]').each do |page|
      puts page
      puts num = num+1
      url_list &lt;&lt; page.text 
    end

    return url_list
  end

  def GetProductURLListAll(url,n)
    url_list = Array.new
    GetURLListAll(url,n).each do |url_tmp|
      doc = GetHtmlObj(url_tmp)
      doc.css('a[class = "history-item product "]').each do |page|
        url_list &lt;&lt; page.attribute("href").text 
      end
    end
    return url_list
  end

 end

# スクレイピング先のURL

class HtmlObjs
  def GetHtmlObj(init_url, n)
    obj = URLList.new
    urls = obj.GetURLListAll(init_url, n)

    htmls = Array.new
    Anemone.crawl(urls, :depth_limit =&gt; 0, :delay =&gt; 1) do |anemone|
          anemone.on_every_page do |page|
            htmls &lt;&lt; page.doc
          end 
    end

    return htmls
  end

  def GetProductHtmlObj(init_url,n)
    obj = URLList.new
    urls = obj.GetProductURLListAll(init_url,n)

    htmls = Array.new
    Anemone.crawl(urls, :depth_limit =&gt; 0, :delay =&gt; 1) do |anemone|
          anemone.on_every_page do |page|
            htmls &lt;&lt; page.doc
          end 
    end

    return htmls
  end

end

class ProductInfo
  def GetProductName(html)
    return html.css("h1[class='product-name']").text
  end
end


init_url = "http://ja.aliexpress.com/wholesale?catId=0&amp;initiative_id=AS_20151205231131&amp;SearchText=bag+mens"
#obj = URLList.new
#puts urls = obj.GetProductURLListAll(init_url,1)



#product = ProductInfo.new


#htmls = HtmlObjs.new
#html_list = htmls.GetProductHtmlObj(init_url,1)


obj = URLList.new

urllist = obj.GetProductURLList(init_url)

#puts urllist



#たまにログイン認証画面を取得してしまう
#商品が36あるはずが、4しか取得できない
```
