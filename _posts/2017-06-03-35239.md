---
layout: post
title: "音声認識の確率的言語モデルに使う辞書の適切な大きさについて"
date: 2017-06-03 01:31:14
categories: 自然言語処理 単語辞書
---
<p>初歩的な音声認識エンジンを作っています。言語モデルの辞書についてうまく考えがまとまらないので、相談させてください。</p>

<p>お尋ねしたいのは音響モデルの発音や言語モデルに使う単語の粒度です。</p>

<p>「たかくらけん」と喋ってみたところ「高倉 建」になりましたが、有名俳優の「高倉健」と聞き取って欲しいと思っています。</p>

<p>形態素解析器に<code>mecab</code>, 辞書に<code>mecab-ipadic-NEologd</code> + <code>ipadic</code>を使っています。<code>neologd</code>は単語数が多く、映画の名前とかゲームの中の登場人物の名前などもエントリに入っている点が素晴らしいと思います。「魔王はベギラゴンを唱えた」のような入力も正しく分割してくれます。</p>

<p>上記の高倉健は、</p>

<ul>
<li><code>ipadic</code> : 高倉, 健 (2語)</li>
<li><code>neologd</code>: 高倉健 (1語)</li>
</ul>

<p>です。</p>

<p>ここで疑問なのですが、<code>n-gram</code>モデルを使って、色々なデータソースから言語モデルを作ると、高倉健は有名人ですので2語に分かれていたとしても出現頻度が高くなるので問題ないような気もします。つまり単語辞書に「高倉健」がある必要はなく「高倉」と「健」さえあれば、確率的言語モデルによって高倉健は正しく認識されるのではないかという予感がします（高倉建や高倉県よりも高倉健のほうが出現頻度が高い）。</p>

<p><code>neologd</code>の辞書をベースに単語辞書を作るとサイズが大きくなります。あまり考えずに作ったところ200MBを超えました。<code>ipadic</code>で作ったときは10MB程度だったと思うので20倍に膨らんだ計算です。</p>

<p><code>neologd</code>では「十二月二十五日」も1つの単語になっています（日付だけで366語ある）。1ドル, 2ドル .... 10000ドルなど数詞も辞書に登録されています。これはかえって言語モデルを作るのにマイナスのようにも思えてきました。<code>neologd</code>を使った場合は「形態素」解析ではなくなっています。</p>

<p>一方で「恋がヘタでも生きてます」というドラマの場合は「生きています」と認識されると困るし「ヘタ」は「下手」でも困ります。これは<code>neologd</code>由来の辞書が威力を発揮するケースです。</p>

<p>ということを自分で考えていたのですが、よい方策が思いつきませんでした。<code>neologd</code>のエントリは自動的に作られた節があり、品詞が正しく設定されていないことがあります。つまり、品詞を手がかりに辞書に組み込む、組み込まないを分類することもできません（人名は含めない、日付は含めない、作品名は含める等）。</p>

<p>私は音声認識の分野は素人です。この分野で学位を持っているわけでもないし、辞書づくり専門のアシスタントがいる大学などの研究機関や学会に所属しているわけでもありません。ですから膨大な単語を自分の手で仕分けすることはできそうにありません。</p>

<p>漠然とした質問ですが、音声認識の統計的言語モデル（<code>n-gram</code>モデル）の辞書作りの指針や経験談のようなものを教えていただければ幸いです。根拠のない勘による予想ですが、あまり辞書が充実しすぎると、本来は2-gramや3-gramで表せばいいものも1-gramになってしまい認識精度が落ちるのではないかと思っています。一方で少なすぎてもいい結果は得られないでしょう。</p>
