---
layout: post
title: 巨大なデータを扱う定期実行処理を、データ量に応じてスケールアウトさせたい
date: 2015-11-24 14:23:24
categories: api batch-file
---
<!-- {% raw %} -->
<h2>状況</h2>

<p>巨大なデータを返すWebAPIを使ったスクリプトを、5分間隔程度の比較的短い頻度で定期的に実行したいと考えています。定期実行するスクリプトはWebAPIからデータをガツっと取得し、そのデータをごにょごにょします。</p>

<p>現時点ではそこまで巨大なデータではないので、1台で捌くことができると考えていますが、今後データ量が大幅に増加していくことが予想されています。また、データ量が増えると5分で処理しきれない可能性が出てきます。<br>
そのため、データ量に応じて柔軟にスケールアウトさせる仕組みを整えておきたいと考えています。具体的に言うと、データが増えてもスクリプトを実行するサーバを増やせば対応できるような構成にしたいと考えています。</p>

<h2>教えていただきたいこと</h2>

<p>このような状況で、何か良い実装方法、ツール、アーキテクチャ等々御存知の方がいましたら、教えていただきたいです。<br>
そもそも根本的な部分の考え方が良くない、というようなものでも構いません。</p>

<p>よろしくお願いいたします。</p>

<h2>補足</h2>

<ul>
<li>WebAPI側にパラメータを増やしたり、機能追加をする方法も可能です</li>
<li>現時点では、WebAPIに全データの件数を返すAPIを用意して、その件数を取得し、バッチ処理を行うサーバの台数で割って、それぞれのサーバが処理する件数を均等にする...というような方法を考えています</li>
</ul>

<h2>追記</h2>

<p>Yuki Inoueさん、Kenji Noguchiさん<br>
コメントありがとうございます。情報が不足しており申し訳ありません。補足いたします。</p>

<blockquote>
  <p>実行する定期実行の処理が、スケールアウトできるものであることが前提であるように思われます。その定期実行で何をやりたいかが定義されないと、回答を行うのは難しいので‌​はないか、と感じました。</p>
</blockquote>

<p>定期実行でやりたいことは、指定されたURLに対してHTTPリクエストをし、その結果をログに吐くジョブをキューに登録することです。ジョブを実行するのは別サーバの役割です。<br>
巨大なデータを返すAPI、といったところからURLの情報を取得します。</p>

<blockquote>
  <p>入力と出力のデータ形式、データ量、途中でどんな計算をするのか‌​を示してください。</p>
</blockquote>

<p>■ 入力と出力<br>
WebAPIは、現時点ではページネーションのパラメータしかありません。pageパラメータで取得するページ番号、per_pageパラメータで1ページの取得件数が指定できます。<br>
WebAPIのレスポンスは、URL情報がJSON形式になったものです。</p>

<pre><code>[
  { "id": 1, "url": "http://example.com"},
  { "id": 2, "url": "http://example.co.jp"},
  ...
]
</code></pre>

<p>※ APIも自前で持っているものなので、機能拡張は可能です。</p>

<p>■ データ量<br>
データ量は正確には見積もれていないのですが、今後増加することだけはわかっており、現状のままでは破綻する気がしており、今回の質問をした次第です。</p>

<p>■ 途中の処理<br>
バッチスクリプトでは、HTTPリクエストを投げるジョブをキューに登録するのみです。</p>
<!-- {% endraw %} -->
