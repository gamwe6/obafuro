---
layout: post
title: "他クラス線形回帰に対する最急降下法"
date: 2018-08-22 06:02:43
categories: python 機械学習
---
<p>タイトル通り線形回帰に対して最急降下法を行いました。もちろん解析的に解は出せるんですが、それはここでは置いておいてください。</p>

<p>目的関数は</p>

<pre><code>$$ \frac{1}{2} \| Xb - y \|_2^2 $$
</code></pre>

<p>であり、その勾配は</p>

<pre><code>$$ X^T X b - X^T y $$
</code></pre>

<p>となります。これを用いて最急降下法を行いましたが、最小化されません。むしろコストが上がっていきます。</p>

<pre><code>import pandas as pd
import numpy as np

data = pd.read_csv('https://raw.githubusercontent.com/mubaris/potential-enigma/master/student.csv')

x = data['Math'].values
y = data['Reading'].values
z = data['Writing'].values

def costf(X, y, param):
    return np.sum((X.dot(param) - y) ** 2)/2.

interc = np.ones(1000)
X = np.concatenate([interc.reshape(-1, 1), x.reshape(-1, 1), y.reshape(-1, 1)], axis=1)

param = np.array([0,0,0])

 def gradient_descent(X, y, param, eta=0.001, iter=10):
    cost_history = [0] * iter

    for iteration in xrange(iter):
        h = X.dot(param)
        loss = h - y
        gradient = X.T.dot(loss)
        param = param - eta * gradient
        cost = costf(X, y, param)
        print cost
        cost_history[iteration] = cost

    return param, cost_history
</code></pre>

<p>どこがおかしいのでしょうか？よろしくお願いします。</p>
