---
layout: post
title: "tensorflowで畳み込みAE作成中、出力データの形がおかしい"
date: 2018-05-24 14:34:58
categories: python python3 tensorflow
---
<pre><code>def conv_encoder(x, weigths, biases):
  x = tf.reshape(x, shape=[-1, 28, 28, 1])
  conv1 = conv2d(x, weigths['ew1'], biases['eb1'])
  pool1 = max_pool2d(conv1)  
  conv2 = conv2d(pool1, weigths['ew2'], biases['eb2'])
  pool2 = max_pool2d(conv2)

  return pool2

def conv_decoder(x, weights, biases):
 deconv1 = deconv2d(x, weights['dw1'], biases['db1'], [batch_size, 14, 14, 32], strides=2)
 deconv2 = deconv2d(deconv1, weights['dw2'], biases['db2'], [batch_size, 28, 28, 16], strides=2)
 output = deconv2d(deconv2, weights['out'], biases['out'], [batch_size, 28, 28, 1], strides=1)

 return output

ew_conv = {
  'ew1': tf.Variable(tf.random_normal([5, 5, 1, 32])),
  'ew2': tf.Variable(tf.random_normal([5, 5, 32, 64]))
}

dw_conv = {
  'dw1': tf.Variable(tf.random_normal([5, 5, 32, 64])),
  'dw2': tf.Variable(tf.random_normal([5, 5, 16, 32])),
  'out': tf.Variable(tf.random_normal([5, 5, 1, 16]))

}

eb_conv = {
  'eb1': tf.Variable(tf.random_normal([32])),
  'eb2': tf.Variable(tf.random_normal([64]))
}
db_conv = {
  'db1': tf.Variable(tf.random_normal([32])),
  'db2': tf.Variable(tf.random_normal([16])),
  'out': tf.Variable(tf.random_normal([1]))
}
</code></pre>

<p>現在，こんな感じでエンコーダとデコーダの定義をして、下記の設定で学習させているのですが，<br>
decoderのoutput.shapeが(batch_size, 28, 28, 784)ってなってしまいます。</p>

<p>outputの行列[256, 28, 28, 1]の形でデータを格納したいです。<br>
しかし，出力結果は，[256, 28, 28, 28^2]と最後の配列に全ての値が格納されています</p>

<pre><code>x = tf.placeholder(tf.float32, shape=([None, num_input]))
encode_op = conv_encoder(x, ew_conv, eb_conv)
decode_op = conv_decoder(encode_op, dw_conv, db_conv)

y_pred = decode_op
y_true = x

loss = tf.reduce_mean(tf.pow(y_pred - y_true, 2))
optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)

init = tf.global_variables_initializer()
with tf.Session() as sess:
  sess.run(init)

  for step in range(1, num_steps+1):
    batch_xs, batch_ys = mnist.train.next_batch(batch_size)

    _, l  = sess.run([optimizer, loss], feed_dict={x:batch_xs})

    if step % display_steps == 0 or step == 1:
      print('Step %i: Minibatch Loss: %f'%(step, l))
</code></pre>

<p>ちなみに、エラー理由は下記のようです。mnistのデータが最後のtensorに全て格納されてしまったため、メモリ容量突破したみたいです</p>

<pre><code> OOM when allocating tensor of shape [256,28,28,784] and type float
</code></pre>
