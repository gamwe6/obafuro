---
layout: post
title: ディープラーニング の学習に必要な専用GPUメモリの計算方法を知りたい。
date: 2019-06-26 02:43:16
categories: python chainer gpu
---
<p>[前提・実現したいこと]</p>

<p>状況によって変わると思いますが、ディープラーニングの学習をGPUを使用して行うのに必要な専用GPUメモリの容量の計算方法を教えて頂きたいです。</p>

<p>とゆうのも、U-netを使用してセグメンテーションを行おうとしているのですが、現在使用しているGPU(GeForce 1050 Ti、専用メモリ:4GB)では私の理想的なミニバッチ数を設定して実行するとOut of memoryとエラーが出てしまいます。そこで、新しいGPUを購入しようと考えているのですが、そのためにはモデルの学習に必要なGPU専用メモリの容量の計算方法を知らないと、どのGPUを買えばいいのか決めることが難しいため、困っております。</p>

<p>[仮の設定]</p>

<ul>
<li>タスク: 分類</li>
<li>トレーニングデータ: 256×256の白黒(1 channel)画像が200枚</li>
<li>テストデータ: その画像のカテゴリーの値(cat, apple, boatなど全10種類)</li>
<li>使用言語: python</li>
<li>フレームワーク: chainer</li>
<li>モデル: VGG16</li>
<li>ミニバッチ数: 20</li>
<li>最適化アルゴリズム: adam</li>
<li>エポック数: 300</li>
</ul>

<p>[私の考え]</p>

<p>ミニバッチ分だけのデータをパラメータの更新のたびにGPU上にコピーしてゆくプログラムの場合、上の設定で必要なメモリは、<br>
20(ミニバッチの数)×画像1枚の容量+VGG16の容量 <br>
これだけだと考えておりました。(ここで、VGG16の容量は学習後のmodelを保存(serializers.save_npzで)した後にできるパラメータの値を保持しているファイルの容量を調べて確認するものとします。)<br>
しかし実際には、タスクマネージャーで確認するとこの計算した容量よりも、もっと多くの容量が使用されていることが確認できました。</p>

<p>これは、私の計算が間違っているのでしょうか？<br>
それとも、モデルのパラメータの更新などの計算のためにも専用GPUメモリが使用されるからでしょうか？もしそうだとしたら、どのように、その計算のために使用されるメモリ量を計算することができるでしょうか？</p>

<p>また、chainerの設定で、実際に必要なGPUのメモリよりも多くのメモリを確保する設定がデフォルトでされているのではないか、とも考えております。そこら辺もご存知でしたら、教えて頂きたいです。</p>
