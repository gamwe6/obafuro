---
layout: post
title: BeautifulSoup4でブログの画像を取得する際に高速化を行いたい。
date: 2017-06-06 04:34:25
categories: python macos python3
---
<!-- {% raw %} -->
<p>Mac OS Sierra<br>
Ver 10.12.1<br>
Python3</p>

<p>取得するページはこちらです。<br>
<a href="http://www.keyakizaka46.com/s/k46o/diary/member/list?ima=0000&amp;page=0&amp;rw=20&amp;cd=member&amp;ct=02" rel="nofollow noreferrer">http://www.keyakizaka46.com/s/k46o/diary/member/list?ima=0000&amp;page=0&amp;rw=20&amp;cd=member&amp;ct=02</a></p>

<p>pageを回すだけでページを巡回することができます。<br>
現在のコードがこちらです。</p>

<pre><code>#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from bs4 import BeautifulSoup
import sys
from urllib import request
from urllib.parse import urlparse
from urllib.error import URLError, HTTPError
import os.path
import os
import time
import re 


# url先の画像を保存する関数
  def download(url):
   url = url
   headers = {
              "User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) 
Gecko/20100101 Firefox/47.0",
                  }
print(url)
req = request.Request(url, None, headers)
img = request.urlopen(req)
localfile = open(os.path.basename(url), 'wb')
localfile.write(img.read())
img.close()
localfile.close()


 # shutterstockの画像検索結果を保存
 # アクセス先遷移
 #par_url = 'http://www.keyakizaka46.com/s/k46o/diary/member/list?
site=k46o&amp;ima=0000&amp;ct=17'
 #imoはページ数 ctはメンバー数
 # def page_info(url):
 #     url = url + "?ima=" + str("0".zfill(4)) + 
"&amp;page=8&amp;rw=20&amp;cd=member&amp;ct=02"
#     return url

 def main():

    for i in range(0,100):
        try:
            url = "http://www.keyakizaka46.com/s/k46o/diary/member/list?ima=0000&amp;page={}&amp;rw=20&amp;cd=member&amp;ct=02".format(str(i))
        # urlアクセス
            res = request.urlopen(url)
       # beautifulsoupでパース
          soup = BeautifulSoup(res.read(),"html.parser")
        # ページに存在するimgタグを検索
        for link in soup.find_all('img'):
       # 画像URLを取得


            img_url = link.get('src')
            if img_url.netloc == "http://www.keyakizaka46.com" and img_url.path.startswith("/images/"):
                print(img_url)

        # ローカルに画像をダウンロード
                download(img_url)
                time.sleep(10)
    except HTTPError as e:
        print('Error code: ', e.code)
    except URLError as e:
        print('Error code: ', e.code)
if __name__ == '__main__':
    main()
</code></pre>

<p>こちらのプログラムを起動して放置したところ２時間半程度で４００枚程度の画像しか保存ができませんでした。<br>
こちらの処理を高速化するためにはどのようにすればよいでしょうか教えてください。</p>
<!-- {% endraw %} -->
