---
layout: post
title: TensorFlowでDQN なぜかQ最大値が小さい
date: 2016-11-11 01:38:04
categories: python 機械学習 tensorflow 深層学習
---
<!-- {% raw %} -->
<p>お世話になっております。</p>

<p>下記の記事を書いている者です。</p>

<p><a href="http://qiita.com/sasaco/items/3b0b8565d6aa2a640caf" rel="nofollow noreferrer">機械学習の理論を理解せずに tensorflow で オセロ AI を作ってみた</a></p>

<p>今回お聞きしたいのは、<br>
上記の オセロ AI の訓練時に Q_max が小さいまま 訓練されない</p>

<p>ソースは上記URLにリンクがあります。(ttps://github.com/sasaco/tf-dqn-reversi.git)</p>

<ul>
<li>train.py　---　AI の訓練を行う</li>
<li>Reversi.py　---　オセロゲームの管理</li>
<li>dqn_agent.py　---　AI の訓練の管理</li>
</ul>

<p>python3:train.py</p>

<pre><code>    players[j].store_experience(state, targets, tr, reword, state_X, target_X, end)
    players[j].experience_replay()
</code></pre>

<p>変数名<br>
- state　---　盤面( = Reversi.screen[0～7][0～7] )  <br>
- targets　---　置いていい番号  <br>
- tr　---　選択した行動  <br>
- reward　---　行動に対する報酬　0～1  <br>
- state_X　---　行動した後の盤面 <br>
- targets_X　---　行動した後の置いていい番号  <br>
- end　---　ゲームが終了＝True  </p>

<p>python3:dqn_agent.py</p>

<pre><code>def store_experience(self, state, targets, action, reward, state_1, targets_1, terminal):
    self.D.append((state, targets, action, reward, state_1, targets_1, terminal))

def experience_replay(self):
    state_minibatch = []
    y_minibatch = []

    # sample random minibatch
    minibatch_size = min(len(self.D), self.minibatch_size)
    minibatch_indexes = np.random.randint(0, len(self.D), minibatch_size)

    for j in minibatch_indexes:
        state_j, targets_j, action_j, reward_j, state_j_1, targets_j_1, terminal = self.D[j]
        action_j_index = self.enable_actions.index(action_j)

        y_j = self.Q_values(state_j)

        if terminal:
            y_j[action_j_index] = reward_j
        else:
            # reward_j + gamma * max_action' Q(state', action')
            qvalue, action = self.select_enable_action(state_j_1, targets_j_1)
            y_j[action_j_index] = reward_j + self.discount_factor * qvalue

        state_minibatch.append(state_j)
        y_minibatch.append(y_j)

    # training
    self.sess.run(self.training, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})

    # for log
    self.current_loss = self.sess.run(self.loss, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})
</code></pre>

<p>下記のように毎ターン更新</p>

<pre><code>    y_j[action_j_index] = reward_j + self.discount_factor * qvalue
    state_minibatch.append(state_j)
    y_minibatch.append(y_j)

# training
self.sess.run(self.training, feed_dict={self.x: state_minibatch, self.y_: y_minibatch})
</code></pre>

<p>しているのに loss がほぼ0 のまま <br>
Q_max も 報酬(reward) 勝ったら1 を与えているのに 0.002 とか小さいのです。</p>

<p>参考にしたのが<br>
<a href="http://blog.algolab.jp/post/2016/08/01/tf-dqn-simple-1/" rel="nofollow noreferrer">超シンプルにTensorFlowでDQN (Deep Q Network) を実装してみる 〜導入編〜 | ALGO GEEKS</a><br>
です。</p>

<p>よろしくお願いいたします。</p>
<!-- {% endraw %} -->
