---
layout: post
title: Apache SparkなどのHadoop技術を使う意味がでてくるデータ量はどれくらいでしょうか？
date: 2015-09-30 12:59:14
categories: scala
---
<p>ログ分析を行うのにApache Sparkを検討しています。しかし、本当にSparkを使うことが他の手段よりも効果的なのかの判断がなかなか難しく悩んでいます。</p>

<p>このサイトでは、１ノードに収まるディスクで対応できるならHadoopなどを使う必要はないという主張がされています。<br>
<a href="https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html" rel="nofollow">https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html</a></p>

<p>現在ではSparkなどの高速なHadoop技術も成長しているので、この記事が書かれたころとは状況は変わっていると思います。<br>
それでもこのサイトが主張するように１つのノードに収まるデータ量であれば集計はMySQLやC/C++などを選択すべきでしょうか。</p>

<p>明確な答えでなくてもかまわないので、自分なりの指標などを持っている方がいらっしゃいましたらお教えいただきたく思います。</p>

<p>お願い致します。</p>

<h3>追記</h3>

<p>質問のデータ量という話からはちょっとズレますが、MySQLで１億レコードのベンチを取ってみました。<br>
<a href="http://qiita.com/devneko/items/56c391e75537db07a515" rel="nofollow">http://qiita.com/devneko/items/56c391e75537db07a515</a><br>
１億レコードともなると簡単な処理でも1クエリ1分以上はかかりそうです。<br>
マシンの性能アップやチューニングで改善すると思いますが、10億レコードくらいになってくると<br>
さすがにMySQLなどで集計するのは辛くなるのではないでしょうか。<br>
ちなみに1億レコードの元データはSQLのプレーンテキストで約10GBほどなので、データ量の話しに直すと10億レコードは100GBほどということになります。<br>
CPUの使用率があがってないので、C/C++などでマルチスレッドで処理した場合はもっと性能が出そうです。</p>
