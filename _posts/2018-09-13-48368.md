---
layout: post
title: Tensorflowを用いたロジスティック回帰で重みがnanになる
date: 2018-09-13 13:52:32
categories: python tensorflow
---
<p>『<a href="http://gihyo.jp/dev/serial/01/machine-learning/0020" rel="nofollow noreferrer">機械学習 はじめよう 第20回</a>』を参考にして、ロジスティック回帰をTensorflowを用いて実装しようと考えていますが、重み・バイアスがすぐにnanになってしまいます。</p>

<p>調べたところ、対数尤度関数のlogが怪しい、ということでlogの中身に小さい数（1e-5など）を足してみたりしましたが変わりませんでした。</p>

<p>Tensorflowにも不慣れですので他の部分にも不具合があるかもしれません。<br>
何が原因と考えられますでしょうか。<br>
ご教授いただければ幸いです。</p>

<p>list: logistic_tf.py</p>

<pre><code>import numpy as np
import tensorflow as tf

def sigmoid(z):
    return 1.0 / (1.0 + tf.exp(-z))

x_d = tf.placeholder(tf.float32, shape=(None, 2), name='x_d')
t_d = tf.placeholder(tf.float32, shape=(None, ), name='t_d')

W = tf.Variable(tf.zeros([2, 1]))
b = tf.Variable(tf.zeros([1]))
t = sigmoid(tf.matmul(x_d, W) + b)

loss = - tf.reduce_sum(t_d * tf.log(t) + (1 - t_d) * tf.log(1 - t))
optimizer = tf.train.GradientDescentOptimizer(0.1)
train = optimizer.minimize(loss)

init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)

# create data set
def h(x, y):
    return 5 * x + 3 * y - 1
x_data = np.random.randn(100, 2).astype('float32')
t_data = np.array([1 if h(x, y) &gt; 0 else 0 for x, y in x_data])

for step in range(200):
    sess.run(train, feed_dict={x_d: x_data, t_d: t_data})
    if step % 10 == 0:
        print(step, sess.run(W), sess.run(b))
        # W = [[5], [3]], b = [-1], expected

sess.close() 
</code></pre>

<p>9/27追記<br>
学習率を0.1から0.01, 0.001と下げたところ、0.01ではnanになりました。0.001ではnanは回避できましたが、期待される値とは別のところで振動してしまいました。損失関数が間違っているということでしょうか。どのように修正すべきでしょうか。</p>

<p>学習率0.001のときの出力</p>

<pre><code>0 [[ 0.33279842]
 [-0.0880626 ]] [-1.4000001]
1000 [[ 1.2017732 ]
 [-0.18360852]] [-2.0046742]
2000 [[ 1.2017722 ] 
 [-0.18360843]] [-2.0046737]
3000 [[ 1.2017732 ]
 [-0.18360852]] [-2.0046742]
4000 [[ 1.2017722 ]
 [-0.18360843]] [-2.0046737]
5000 [[ 1.2017732 ]
 [-0.18360852]] [-2.0046742]
6000 [[ 1.2017722 ]
 [-0.18360843]] [-2.0046737]
7000 [[ 1.2017732 ]
 [-0.18360852]] [-2.0046742]
8000 [[ 1.2017722 ]
 [-0.18360843]] [-2.0046737]
9000 [[ 1.2017732 ]
 [-0.18360852]] [-2.0046742]
</code></pre>
