---
layout: post
title: "kerasで同じコードでIndexErrorとAttributeErrorがでます"
date: 2018-10-22 09:53:52
categories: python3 機械学習 keras
---
<p>自力で単純なautoencoderを頑張って書いてみました。<br>
しかし、</p>

<pre><code>C:\Users\yudai\Desktop\keras_AE.py:62: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  autoencoder = Model(input=input_word, output=decoded)
Traceback (most recent call last):
  File "C:\Users\yudai\Desktop\keras_AE.py", line 70, in &lt;module&gt;
    shuffle=False)
  File "C:\Users\yudai\Anaconda3\envs\pyMLgpu\lib\site-packages\keras\engine\training.py", line 1039, in fit
    validation_steps=validation_steps)
  File "C:\Users\yudai\Anaconda3\envs\pyMLgpu\lib\site-packages\keras\engine\training_arrays.py", line 139, in fit_loop
    if issparse(ins[i]) and not K.is_sparse(feed[i]):
IndexError: list index out of range
</code></pre>

<p>と出力されます。<br>
もし原因がわかる方がいらっしゃるならば、<br>
何卒、ご教授宜しくお願い致します。<br>
（<a href="https://teratail.com/questions/153594" rel="nofollow noreferrer">Terateilでも質問</a>しています）</p>

<p>追記:<br>
<a href="https://github.com/keras-team/keras/issues/7602" rel="nofollow noreferrer">https://github.com/keras-team/keras/issues/7602</a><br>
より</p>

<pre><code>autoencoder = Model(input=input_word, output=decoded)
</code></pre>

<p>を</p>

<pre><code>autoencoder = Model(inputs=input_word, outputs=decoded)
</code></pre>

<p>に直しました。<br>
しかし、同じエラーが出ます。<br>
いくつか他のエラーがありましたが、修正しました。</p>

<p>入力データの問題かどうかを調べるため、下に示しているC:\Users\hoge\Desktop\poem.txtのように漢字空間あり、漢字空間なし、ひらがな空間あり、ひらがな空間なし、また、それぞれ行替えでありなしを試してみましたが、すべて同じエラーがでます。</p>

<p>違うWindows 10のPCでは、<br>
python 3.6.5<br>
tensorflow 1.8.0<br>
keras 2.1.5</p>

<pre><code>C:\Users\hoge\Desktop\keras_AE.py:62: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor("in..., outputs=Tensor("de...)`
  autoencoder = Model(input=input_word, output=decoded)
Traceback (most recent call last):
  File "C:\Users\hoge\Desktop\keras_AE.py", line 70, in &lt;module&gt;
    shuffle=False)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\keras\engine\training.py", line 1630, in fit
    batch_size=batch_size)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\keras\engine\training.py", line 1487, in _standardize_user_data
    in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\keras\engine\training.py", line 1486, in &lt;listcomp&gt;
    for (ref, sw, cw, mode)
  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Anaconda3_64\lib\site-packages\keras\engine\training.py", line 540, in _standardize_weights
    return np.ones((y.shape[0],), dtype=K.floatx())
AttributeError: 'NoneType' object has no attribute 'shape'
</code></pre>

<p>が同じコードで違うエラーがでます。</p>

<pre><code># -*- coding: utf-8 -*-
from keras.layers import Input, Dense
from keras.models import Model
from keras.utils.data_utils import get_file
import numpy as np
import codecs

#データの前処理
#データの読み込み
with codecs.open(r'C:\Users\yudai\Desktop\poem.txt', 'r', 'utf-8') as f:
    for text in f:
        text = text.strip()
#コーパスの長さ
print('corpus length:', len(text))
#文字数を数えるため、textをソート
chars = sorted(list(set(text)))
#全文字数の表示
print('total chars:', len(chars))
#文字をID変換
char_indices = dict((c, i) for i, c in enumerate(chars))
#IDから文字へ変換
indices_char = dict((i, c) for i, c in enumerate(chars))
#テキストを17文字ずつ読み込む
maxlen = 17
#サンプルバッチ数
step = 3
sentences = []
next_chars = []
for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i + maxlen])
    next_chars.append(text[i + maxlen])
#学習する文字数を表示
print('Sequences:', len)

#ベクトル化する
print('Vectorization...')
x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        x[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

#モデルを構築する工程に入る
print('Build model...')
#encoderの次元
encoding_dim = 128
#入力用の変数
input_word = Input(shape=(x ,y))
#入力された語がencodeされたものを格納する
encoded_h1 = Dense(128, activation='relu')(input_word)
encoded_h2 = Dense(64, activation='relu')(encoded_h1)
encoded_h3 = Dense(32, activation='relu')(encoded_h2)
#潜在変数（実質的な主成分分析）
latent = Dense(8, activation='relu')(encoded_h3)
#encodeされたデータを再構成
decoded_h1 = Dense(32, activation='relu')(latent)
decoded_h2 = Dense(64, activation='relu')(decoded_1)
decoded_h3 = Dense(128, activation='relu')(encoded_2)

output = Dense(100, activation)

autoencoder = Model(inputs=input_word, outputs=decoded)
#Adamで最適化、loss関数をcategorical_crossentropy
autoencoder.compile(optimizer='Adam', 
loss='categorical_crossentropy')

#autoencoderの実行
autoencoder.fit(x_train,
                epoch=1000,
                batch_size=256,
                shuffle=False)
#学習の進み具合を観察
def on_epoch_end(epoch):
    print()
    print('Epoch: %d' % epoch)

#モデルの構造を保存
model_json = autoencoder.to_json()
with open('keras_AE.json', 'w') as json_file:
    json_file.write(model_json)
#学習済みモデルの重みを保存
autoencoder.save_weights('AE.h5')

decoded_word = autoencoder.predict(word_test)

X_embedded = model.predict(X_train)
autoencoder.fit(X_embedded,X_embedded,epochs=10,
            batch_size=256, validation_split=.1)
</code></pre>

<p>C:\Users\hoge\Desktop\poem.txtは、webから２万９０００件の俳句を一文ずつ抽出し、MeCabで形態素解析を行っています。<br>
例：<br>
朝霧 の 中 に 九段 の ともし 哉<br>
あたたか な 雨 が 降る なり 枯葎<br>
菜の花 や は つと 明るき 町 は づれ<br>
秋風 や 伊予 へ 流る る 汐 の 音<br>
長閑 さ や 障子 の 穴 に 海 見え て<br>
若鮎 の 二 手 に なりて 上り けり<br>
行く 秋 を す つく と 鹿 の 立ち に けり<br>
我 声 の 風 に なり けり 茸狩<br>
毎年 よ 彼岸の入り に 寒い の は</p>

<h1>環境</h1>

<p>Windows 10</p>

<p>python 3.7.0<br>
tensorflow-gpu 1.9.0<br>
keras 2.2.4</p>
