---
layout: post
title: chainerのsoftmax_cross_entropy()について
date: 2015-06-19 07:19:40
categories: python 機械学習
---
<p>pythonとも機械学習とも勉強不足でわからない点があったため、chainerの交差エントロピー誤差を計算するsoftmax_cross_entropy() について質問させてください。</p>

<p>MNISTを識別するサンプルコード<br>
<a href="https://github.com/pfnet/chainer/blob/master/examples/mnist/train_mnist.py" rel="nofollow">https://github.com/pfnet/chainer/blob/master/examples/mnist/train_mnist.py</a><br>
を動かしてみました。</p>

<p>疑問に思った点は以下です。</p>

<ol>
<li><p>49行目のreturn F.softmax_cross_entropy(y, t), F.accuracy(y, t) で、多クラス識別をする際の交差エントロピー誤差は、出力層のユニット数分(ラベルに対応するユニットだけでなくほかのユニットの確率も余事象として)計算しなければならないのに、教師データtを1ofK表記にせず、そのまま渡している点。<br>
交差エントロピーは与えられたラベルのユニットだけに関する重みを更新するのでしょうか。<br>
関数内部で1ofK表記、または多ラベル表記に変換しているのかなと考えてソースを見たのですがよくわかりませんでした。どのような処理をしているのかも教えてほしいです。</p></li>
<li><p>48行目の y  = model.l3(h2) でsoftmax関数を通していない理由は、softmax_cross_entropy() 内で　self.y, = Softmax().forward_cpu((x,)) として、関数内でsoftmax関数を適用しているからなのでしょうか<br>
そうすると 49行目の return F.softmax_cross_entropy(y, t), F.accuracy(y, t) のyをh2の値を渡してもいい気がするのですがどうなんでしょう(yで線形変換する必要はあるのか)</p></li>
</ol>

<p>質問は以上です。<br>
初歩的な質問かもしれませんがよろしくお願いします。</p>

<p>交差エントロピー、多クラス識別について参考にしたブログ<br>
<a href="http://hshinji.hateblo.jp/entry/2015/05/20/081530" rel="nofollow">http://hshinji.hateblo.jp/entry/2015/05/20/081530</a></p>

<p>softmax_cross_entropyのReference<br>
<a href="https://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py" rel="nofollow">https://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py</a></p>
