---
layout: post
title: "Amazon EMRでS3に書き込みの際に503 Slow Downが発生する"
date: 2019-03-14 01:51:44
categories: amazon-s3 spark pyspark
---
<p>Amazon EMRでPySparkを動かしています。<br>
その際にS3にparquetで保存する処理中にAmazonS3Exceptionが発生致します。</p>

<p>コードは以下の通りです。</p>

<pre><code>s3_path = 's3://hoge/huga/'
df.write.format('parquet').mode('overwrite').save(s3_path)
</code></pre>

<p>インスタンスはマスタノード、コアノード共に、r3.2xlargeになります。<br>
元々r3.4xlargeやr3.8xlargeで動作させていたのですが、<br>
同エラーが多発したため一旦r3.2xlargeに落として動作させているという事情があります。<br>
（数十回は発生していなかったためこれでいけると思ったのですが再発し、根本解決のため質問させていただいた次第です。）</p>

<p>データフレームのデータ量はかなりの量があります。</p>

<p>調べたところ、徐々にリクエストを増やすか、プレフィックスを付けることで解決できそうなことはわかっております。<br>
<a href="https://aws.amazon.com/jp/premiumsupport/knowledge-center/http-5xx-errors-s3/" rel="nofollow noreferrer">https://aws.amazon.com/jp/premiumsupport/knowledge-center/http-5xx-errors-s3/</a></p>

<p>が、PySparkでどのように設定をする事で上記が解決できるかわからなかったため、教えてください。</p>

<p>情報の不足等ありましたらコメント頂ければと思います。<br>
よろしくお願いいたします。</p>
